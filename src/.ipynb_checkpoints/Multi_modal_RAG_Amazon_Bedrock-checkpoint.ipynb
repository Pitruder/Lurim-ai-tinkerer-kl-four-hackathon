{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Multimodal RAG with Amazon Bedrock\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Multimodal RAG with Amazon Bedrock](../img/multimodal-rag1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Importing the libs\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (1.35.39)\n",
      "Requirement already satisfied: botocore in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (1.35.39)\n",
      "Collecting pypdfium2==4.30.0 (from -r requirements.txt (line 3))\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Collecting faiss-cpu==1.8.0.post1 (from -r requirements.txt (line 4))\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting PyMuPDF==1.24.10 (from -r requirements.txt (line 5))\n",
      "  Downloading PyMuPDF-1.24.10-cp312-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting duckduckgo_search (from -r requirements.txt (line 6))\n",
      "  Downloading duckduckgo_search-6.3.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain==0.2.16 (from -r requirements.txt (line 7))\n",
      "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community==0.2.17 (from -r requirements.txt (line 8))\n",
      "  Downloading langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-aws==0.1.18 (from -r requirements.txt (line 9))\n",
      "  Downloading langchain_aws-0.1.18-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: tabula in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 11)) (4.66.5)\n",
      "Collecting pillow (from -r requirements.txt (line 12))\n",
      "  Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting matplotlib (from -r requirements.txt (line 13))\n",
      "  Downloading matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu==1.8.0.post1->-r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu==1.8.0.post1->-r requirements.txt (line 4)) (24.1)\n",
      "Collecting PyMuPDFb==1.24.10 (from PyMuPDF==1.24.10->-r requirements.txt (line 5))\n",
      "  Downloading PyMuPDFb-1.24.10-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.2.16->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.2.16->-r requirements.txt (line 7)) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.2.16->-r requirements.txt (line 7)) (3.10.10)\n",
      "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain==0.2.16->-r requirements.txt (line 7))\n",
      "  Downloading langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.16->-r requirements.txt (line 7))\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.2.16->-r requirements.txt (line 7)) (0.1.134)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.2.16->-r requirements.txt (line 7)) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.2.16->-r requirements.txt (line 7)) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain==0.2.16->-r requirements.txt (line 7)) (8.5.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.2.17->-r requirements.txt (line 8))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting boto3 (from -r requirements.txt (line 1))\n",
      "  Downloading boto3-1.34.162-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore (from -r requirements.txt (line 2))\n",
      "  Downloading botocore-1.34.162-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from boto3->-r requirements.txt (line 1)) (0.10.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore->-r requirements.txt (line 2)) (2.2.3)\n",
      "Collecting click>=8.1.7 (from duckduckgo_search->-r requirements.txt (line 6))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting primp>=0.6.3 (from duckduckgo_search->-r requirements.txt (line 6))\n",
      "  Downloading primp-0.6.3-cp38-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tabula->-r requirements.txt (line 10)) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->-r requirements.txt (line 11)) (0.4.6)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 13))\n",
      "  Downloading contourpy-1.3.0-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 13))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 13))\n",
      "  Downloading fonttools-4.54.1-cp312-cp312-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 13))\n",
      "  Downloading kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 13))\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16->-r requirements.txt (line 7)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16->-r requirements.txt (line 7)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16->-r requirements.txt (line 7)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16->-r requirements.txt (line 7)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.16->-r requirements.txt (line 7)) (1.15.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.17->-r requirements.txt (line 8))\n",
      "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.17->-r requirements.txt (line 8))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain==0.2.16->-r requirements.txt (line 7)) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain==0.2.16->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.16->-r requirements.txt (line 7)) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.16->-r requirements.txt (line 7)) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.16->-r requirements.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.16->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain==0.2.16->-r requirements.txt (line 7)) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.2.16->-r requirements.txt (line 7)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.2.16->-r requirements.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain==0.2.16->-r requirements.txt (line 7)) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.16->-r requirements.txt (line 7)) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.16->-r requirements.txt (line 7)) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.16->-r requirements.txt (line 7)) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.16->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.16->-r requirements.txt (line 7)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain==0.2.16->-r requirements.txt (line 7)) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.17->-r requirements.txt (line 8))\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp probook\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain==0.2.16->-r requirements.txt (line 7)) (0.2.0)\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.3/2.9 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading faiss_cpu-1.8.0.post1-cp312-cp312-win_amd64.whl (14.6 MB)\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/14.6 MB 8.4 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.4/14.6 MB 5.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.1/14.6 MB 5.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.1/14.6 MB 5.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.2/14.6 MB 3.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.2/14.6 MB 3.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.3/14.6 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.3/14.6 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 7.3/14.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.4/14.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.4/14.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.2/14.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.0/14.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.6/14.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.6/14.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.4/14.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.6/14.6 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading PyMuPDF-1.24.10-cp312-none-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 1.0/3.2 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.6/3.2 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.1/3.2 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 7.9 MB/s eta 0:00:00\n",
      "Downloading langchain_community-0.2.17-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.3/2.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading langchain_aws-0.1.18-py3-none-any.whl (83 kB)\n",
      "Downloading PyMuPDFb-1.24.10-py3-none-win_amd64.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/13.2 MB 5.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.9/13.2 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.7/13.2 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.6/13.2 MB 7.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.9/13.2 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.2/13.2 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.1/13.2 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading boto3-1.34.162-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.34.162-py3-none-any.whl (12.5 MB)\n",
      "   ---------------------------------------- 0.0/12.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.8/12.5 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.5 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.0/12.5 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.5 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.2/12.5 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.5 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.5/12.5 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading duckduckgo_search-6.3.0-py3-none-any.whl (27 kB)\n",
      "Downloading pillow-10.4.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.4/2.6 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.3/7.8 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.9/7.8 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.7/7.8 MB 7.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.6/7.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading contourpy-1.3.0-cp312-cp312-win_amd64.whl (218 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading fonttools-4.54.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Downloading langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
      "Downloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Downloading primp-0.6.3-cp38-abi3-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.8/2.8 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 8.4 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: pypdfium2, pyparsing, PyMuPDFb, primp, pillow, mypy-extensions, marshmallow, kiwisolver, fonttools, faiss-cpu, cycler, contourpy, click, typing-inspect, PyMuPDF, matplotlib, duckduckgo_search, botocore, dataclasses-json, langchain-core, boto3, langchain-text-splitters, langchain-aws, langchain, langchain-community\n",
      "  Attempting uninstall: faiss-cpu\n",
      "    Found existing installation: faiss-cpu 1.9.0\n",
      "    Uninstalling faiss-cpu-1.9.0:\n",
      "      Successfully uninstalled faiss-cpu-1.9.0\n",
      "  Attempting uninstall: PyMuPDF\n",
      "    Found existing installation: PyMuPDF 1.24.11\n",
      "    Uninstalling PyMuPDF-1.24.11:\n",
      "      Successfully uninstalled PyMuPDF-1.24.11\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.39\n",
      "    Uninstalling botocore-1.35.39:\n",
      "      Successfully uninstalled botocore-1.35.39\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.10\n",
      "    Uninstalling langchain-core-0.3.10:\n",
      "      Successfully uninstalled langchain-core-0.3.10\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.39\n",
      "    Uninstalling boto3-1.35.39:\n",
      "      Successfully uninstalled boto3-1.35.39\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.0\n",
      "    Uninstalling langchain-text-splitters-0.3.0:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.3\n",
      "    Uninstalling langchain-0.3.3:\n",
      "      Successfully uninstalled langchain-0.3.3\n",
      "Successfully installed PyMuPDF-1.24.10 PyMuPDFb-1.24.10 boto3-1.34.162 botocore-1.34.162 click-8.1.7 contourpy-1.3.0 cycler-0.12.1 dataclasses-json-0.6.7 duckduckgo_search-6.3.0 faiss-cpu-1.8.0.post1 fonttools-4.54.1 kiwisolver-1.4.7 langchain-0.2.16 langchain-aws-0.1.18 langchain-community-0.2.17 langchain-core-0.2.41 langchain-text-splitters-0.2.4 marshmallow-3.22.0 matplotlib-3.9.2 mypy-extensions-1.0.0 pillow-10.4.0 primp-0.6.3 pyparsing-3.1.4 pypdfium2-4.30.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _extra: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymupdf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymupdf\\__init__.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extra\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Set up g_out_log and g_out_message from environment variables.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# PYMUPDF_MESSAGE controls the destination of user messages (the `message()`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# stdout.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_stream\u001b[39m(name, default):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymupdf\\extra.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Import the low-level C/C++ module\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __package__ \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extra\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m_extra\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _extra: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "#try venv\n",
    "import boto3\n",
    "import tabula\n",
    "import faiss\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import pymupdf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Data Loading\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"attention_paper.pdf\"\n",
    "filepath = \"data/\" + filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"data/attention_paper.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x146598cb5c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"attention_paper.pdf\"\n",
    "filepath = \"data/\" + filename\n",
    "display.IFrame(filepath, width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Data Extraction\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: utils\n",
      "  Building wheel for utils (pyproject.toml): started\n",
      "  Building wheel for utils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13936 sha256=4436cd4e9dcfaabcd2e2593c936d52e8ebdce43deabf859d204a611cfe88dcb9\n",
      "  Stored in directory: c:\\users\\hp probook\\appdata\\local\\pip\\cache\\wheels\\b6\\a1\\81\\1036477786ae0e17b522f6f5a838f9bc4288d1016fc5d0e1ec\n",
      "Successfully built utils\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement utils.utils (from versions: none)\n",
      "ERROR: No matching distribution found for utils.utils\n"
     ]
    }
   ],
   "source": [
    "!pip install utils.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.util'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pdf2imgs\n\u001b[0;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m pymupdf\u001b[38;5;241m.\u001b[39mopen(filepath)\n\u001b[0;32m      4\u001b[0m num_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(doc)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils.util'"
     ]
    }
   ],
   "source": [
    "from utils.utils import pdf2imgs\n",
    "\n",
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "\n",
    "# Define the directories to store the extracted text, images and page images from each page\n",
    "image_save_dir = \"data/processed_images\"\n",
    "text_save_dir = \"data/processed_text\"\n",
    "table_save_dir = \"data/processed_tables\"\n",
    "page_images_save_dir = \"data/processed_page_images\"\n",
    "\n",
    "# Chunk the text for effective retrieval\n",
    "chunk_size = 700\n",
    "overlap=200\n",
    "\n",
    "# Process chunks with LangChain's RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=overlap,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Process all pages of the PDF\n",
    "items = []\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "\n",
    "    # Step 1: Extract tables using Tabula\n",
    "    try:\n",
    "        tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "        if tables:\n",
    "            for table_idx, table in enumerate(tables):\n",
    "                # Convert the table DataFrame to a string format (Markdown-style for clarity)\n",
    "                table_text = \"\\n\".join([\" | \".join(map(str, row)) for row in table.values])\n",
    "\n",
    "                # Save the table text as a file\n",
    "                table_file_name = f\"{table_save_dir}/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
    "                os.makedirs(table_save_dir, exist_ok=True)\n",
    "                with open(table_file_name, 'w') as f:\n",
    "                    f.write(table_text)\n",
    "\n",
    "                # Add table information to items (format as a string for embeddings)\n",
    "                table_item = {\n",
    "                    \"page\": page_num,\n",
    "                    \"type\": \"table\",\n",
    "                    \"text\": table_text,  # Use the formatted table text here\n",
    "                    \"path\": table_file_name\n",
    "                }\n",
    "                items.append(table_item)\n",
    "\n",
    "                # Optionally remove table text from the page's text to avoid duplication\n",
    "                text = text.replace(table_text, \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from page {page_num}: {str(e)}\")\n",
    "            \n",
    "    # Process chunks with overlap\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    # chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size-overlap)]\n",
    "    \n",
    "    # Generate an item to add to items\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        text_file_name = f\"{text_save_dir}/{filename}_text_{page_num}_{i}.txt\"\n",
    "        # If the text folder doesn't exist, create one\n",
    "        os.makedirs(text_save_dir, exist_ok=True)\n",
    "        with open(text_file_name, 'w') as f:\n",
    "            f.write(chunk)\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"text\"\n",
    "        item[\"text\"] = chunk\n",
    "        item[\"path\"] = text_file_name\n",
    "        items.append(item)\n",
    "    \n",
    "    \n",
    "    # Get all the images in the current page\n",
    "    images = page.get_images()\n",
    "    for idx, image in enumerate(images):        \n",
    "        # Extract the image data\n",
    "        xref = image[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref)\n",
    "        pix.tobytes(\"png\")\n",
    "        # Create the image_name that includes the image path\n",
    "        image_name = f\"{image_save_dir}/{filename}_image_{page_num}_{idx}_{xref}.png\"\n",
    "        # If the image folder doesn't exist, create one\n",
    "        os.makedirs(image_save_dir, exist_ok=True)\n",
    "        # Save the image\n",
    "        pix.save(image_name)\n",
    "        \n",
    "        # Produce base64 string\n",
    "        with open(image_name, 'rb') as f:\n",
    "            image = base64.b64encode(f.read()).decode('utf8')\n",
    "        \n",
    "        item={}\n",
    "        item[\"page\"] = page_num\n",
    "        item[\"type\"] = \"image\"\n",
    "        item[\"path\"] = image_name\n",
    "        item[\"image\"] = image\n",
    "        items.append(item)\n",
    "\n",
    "# Save pdf pages as images\n",
    "page_images_save_dir = pdf2imgs(filepath, page_images_save_dir)\n",
    "\n",
    "for page_num in range(num_pages):\n",
    "    page_path = os.path.join(page_images_save_dir,  f\"page_{page_num:03d}.png\")\n",
    "    \n",
    "    # Produce base64 string\n",
    "    with open(image_name, 'rb') as f:\n",
    "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "    \n",
    "    item = {}\n",
    "    item[\"page\"] = page_num\n",
    "    item[\"type\"] = \"page\"\n",
    "    item[\"path\"] = page_path\n",
    "    item[\"image\"] = page_image\n",
    "    items.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Generating Multimodal Embeddings\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding Generation Code\n",
    "def generate_multimodal_embeddings(prompt=None, image=None, output_embedding_length=384):\n",
    "    \"\"\"\n",
    "    Invoke the Amazon Titan Multimodal Embeddings model using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        image (str): A base64-encoded image data.\n",
    "    Returns:\n",
    "        str: The model's response embedding.\n",
    "    \"\"\"\n",
    "    if not prompt and not image:\n",
    "        raise ValueError(\"Please provide either a text prompt, base64 image, or both as input\")\n",
    "    \n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"amazon.titan-embed-image-v1\"\n",
    "    \n",
    "    body = {\"embeddingConfig\": {\"outputEmbeddingLength\": output_embedding_length}}\n",
    "    \n",
    "    if prompt:\n",
    "        body[\"inputText\"] = prompt\n",
    "    if image:\n",
    "        body[\"inputImage\"] = image\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(body),\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result.get(\"embedding\")\n",
    "\n",
    "    except ClientError as err:\n",
    "        print(f\"Couldn't invoke Titan embedding model. Error: {err.response['Error']['Message']}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|████████████████████| 85/85 [00:23<00:00,  3.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set embedding vector dimension\n",
    "embedding_vector_dimension = 384\n",
    "\n",
    "# Generate embeddings for all items\n",
    "for item in tqdm(items, desc=\"Generating embeddings\"):\n",
    "    if item['type'] == 'text' or item['type'] == 'table':\n",
    "        # For text or table, use the formatted text representation\n",
    "        item['embedding'] = generate_multimodal_embeddings(prompt=item['text'], output_embedding_length=embedding_vector_dimension)\n",
    "    else:\n",
    "        # For images, use the base64-encoded image data\n",
    "        item['embedding'] = generate_multimodal_embeddings(image=item['image'], output_embedding_length=embedding_vector_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_embeddings = np.array([item['embedding'] for item in items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85, 384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Creating Vector Database\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create FAISS Index\n",
    "index = faiss.IndexFlatL2(embedding_vector_dimension)\n",
    "\n",
    "# Clear any pre-existing index\n",
    "index.reset()\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(np.array(all_embeddings, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Generating RAG response with Claude 3\n",
    "def invoke_claude_3_multimodal(prompt, images, image_types):\n",
    "    \"\"\"\n",
    "    Invoke the Claude-3 multimodal model from Anthropic using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        images (list): A list of base64-encoded image data.\n",
    "        image_types (list): A list of MIME types corresponding to the images.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model name is provided.\n",
    "    \"\"\"\n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Prepare the multimodal prompt message\n",
    "    message_content = []\n",
    "\n",
    "    # Add each image to the message content\n",
    "    for image, img_type in zip(images, image_types):\n",
    "        message_content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": img_type,\n",
    "                \"data\": image,\n",
    "            },\n",
    "        })\n",
    "    message_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 250,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message_content,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body),\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result['content'][0]['text']\n",
    "\n",
    "    except ClientError as err:\n",
    "        logger.error(\n",
    "            \"Couldn't invoke Claude 3 %s model. Here's why: %s: %s\",\n",
    "            model_id.split('.')[-1].capitalize(),\n",
    "            err.response[\"Error\"][\"Code\"],\n",
    "            err.response[\"Error\"][\"Message\"],\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_rag_response(prompt, matched_items):\n",
    "    \n",
    "    # Create context\n",
    "    text_context = \"\"\n",
    "    image_context = []\n",
    "    \n",
    "    for item in matched_items:\n",
    "        if item['type'] == 'text':\n",
    "            text_context += str(item[\"page\"]) + \". \" + item['text'] + \"\\n\"\n",
    "        else:\n",
    "            image_context.append(item['image'])\n",
    "    \n",
    "    # Only 5 images are supported by Claude3 models\n",
    "    if len(image_context) > 5:\n",
    "        image_context = image_context[:5]\n",
    "    \n",
    "    final_prompt = f\"\"\"You are a helpful assistant for question answering.\n",
    "    The text context is relevant information retrieved.\n",
    "    The provided image(s) are relevant information retrieved.\n",
    "    \n",
    "    <context>\n",
    "    {text_context}\n",
    "    </context>\n",
    "    \n",
    "    Answer the following question using the relevant context and images.\n",
    "    \n",
    "    <question>\n",
    "    {prompt}\n",
    "    </question>\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    return invoke_claude_3_multimodal(final_prompt, image_context, ['image/png' for _ in image_context])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Test the RAG Pipeline\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "query = \"How is the scaled-dot-product attention is calculated?\"\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 19, 20, 23, 40])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the provided context, the scaled dot-product attention is calculated as follows:\n",
       "\n",
       "Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
       "\n",
       "Where:\n",
       "\n",
       "- Q is the matrix of queries \n",
       "- K is the matrix of keys\n",
       "- V is the matrix of values\n",
       "- d_k is the dimension of the keys\n",
       "\n",
       "The key steps are:\n",
       "\n",
       "1. Take the dot product of the query matrix Q with the transpose of the key matrix K^T. This gives a score showing the similarity between each query and key.\n",
       "\n",
       "2. Scale the scores by dividing by sqrt(d_k), where d_k is the dimension of the keys. This helps prevent the softmax function from being pushed into regions with extremely small gradients when d_k is large.\n",
       "\n",
       "3. Apply the softmax function to the scaled scores to obtain the attention weights.\n",
       "\n",
       "4. Multiply the attention weights with the value matrix V to get the weighted sum of the values, which are the attended outputs.\n",
       "\n",
       "This scaled dot-product attention mechanism allows efficiently computing attention by taking advantage of highly optimized matrix multiplication operations. It differs from additive attention by using the scaled dot-product similarity instead of a feed-forward network to compute attention scores."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to Table 2 in the provided context, the Transformer (big) model achieves a BLEU score of 28.4 on the WMT 2014 English-to-German (EN-DE) translation task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the BLEU score of the model in English to German translation (EN-DE)?\"\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)\n",
    "result.flatten()\n",
    "\n",
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "other_queries = [\"How long were the base and big models trained?\",\n",
    "                 \"Which optimizer was used when training the models?\",\n",
    "                 \"What is the position-wise feed-forward neural network mentioned in the paper?\",\n",
    "                 \"What is the BLEU score of the model in English to French translation (EN-FR)?\",\n",
    "                 \"What is the BLEU score of the model in English to German translation (EN-DE)?\",\n",
    "                 \"How is the scaled-dot-product attention is calculated?\",\n",
    "                 \"What is the BLEU score of the model in English to French translation (EN-FR)?\",\n",
    "                 \"What is the BLEU score of the model in English to German translation (EN-DE)?\",\n",
    "                 \"How is the scaled-dot-product attention is calculated?\",\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the context, the base Transformer models were trained for a total of 100,000 steps or 12 hours, while the big Transformer models were trained for 300,000 steps or 3.5 days (on 8 P100 GPUs). Specifically, the context states:\n",
       "\n",
       "\"We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\"\n",
       "\n",
       "So the base models took 12 hours of training, while the bigger models took 3.5 days of training on 8 GPUs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = other_queries[0]\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)\n",
    "result.flatten()\n",
    "\n",
    "# Retrieve the matched items\n",
    "matched_items = [items[index] for index in result.flatten()]\n",
    "\n",
    "# Generate a response using the RAG pipeline\n",
    "response = generate_rag_response(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Learn More\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/github.png\" alt=\"Multimodal RAG with Amazon Bedrock\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Let's Connect\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"../img/linkedin.png\" alt=\"LinkedIn\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Thank you!\n",
    "</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
